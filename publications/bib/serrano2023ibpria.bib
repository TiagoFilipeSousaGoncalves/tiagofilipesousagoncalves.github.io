@inproceedings{serrano2023ibpria,
    author={Serrano e Silva, Pedro and Cruz, Ricardo and Shihavuddin, A. S. M. and Gon{\c{c}}alves, Tiago},
    editor={Pertusa, Antonio and Gallego, Antonio Javier and S{\'a}nchez, Joan Andreu and Domingues, In{\^e}s},
    title={{Interpretability-Guided Human Feedback During Neural Network Training}},
    booktitle={{Pattern Recognition and Image Analysis}},
    year={2023},
    publisher={{Springer Nature Switzerland}},
    address={{Cham}},
    pages={276--287},
    abstract={When models make wrong predictions, a typical solution is to acquire more data related to the error: an expensive process known as active learning. Our supervised classification approach combines active learning with interpretability so the user can correct such mistakes during the model's training. At the end of each epoch, our training pipeline shows examples of mistaken cases to the user, using interpretability to allow the user to visualise which regions of the images are receiving the model's attention. The user can then guide the training through a regularisation term in the loss function. This approach differs from previous works where the user's role was to annotate unlabelled data since, in this proposal, the user directly influences the training procedure through the loss function. Overall, in low-data regimens, the proposed method returned lower loss values in the predictions made for all three datasets used: 0.61, 0.47, 0.36, when compared with fully automated training methods using the same amount of data: 0.63, 0.52, 0.41, respectively. We also observed higher accuracy values in two datasets: 81.14{\%} and 92.58{\%} over the 78.41{\%} and 92.52{\%} seen in fully automated methods.},
    isbn={978-3-031-36616-1}
}