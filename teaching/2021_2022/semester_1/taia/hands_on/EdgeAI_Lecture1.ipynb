{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EdgeAI_Lecture1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNX3tm4cAdyj"
      },
      "source": [
        "# Edge AI - Lecture 1 (Hands-on)\n",
        "# TAIA - Advanced Topics on Artificial Intelligence\n",
        "# Tiago Filipe Sousa Gon√ßalves\n",
        "# tiago.f.goncalves@inesctec.pt | tiagofs@fe.up.pt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcFfIgXbBGbb"
      },
      "source": [
        "# Contents\n",
        "\n",
        "\n",
        "## 1.   Model Quantisation in TensorFlow & Keras\n",
        "## 2.   Model Pruning in TensorFlow & Keras\n",
        "## 3.   What if we want to jointly use pruning and quantisation?\n",
        "## 4.   More tutorials, exercises and readings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPeAiaYEBVWb"
      },
      "source": [
        "# Model Quantisation in TensorFlow & Keras\n",
        "Theory:\n",
        "https://www.tensorflow.org/model_optimization/guide/quantization/training?hl=en\n",
        "\n",
        "Code and Exercises adapted from:\n",
        "https://www.tensorflow.org/model_optimization/guide/quantization/training_example?hl=en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVqdIQZRKPqO"
      },
      "source": [
        "## Setup\n",
        "Let's start with the setup of our development environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWHS0xq3_Zzv"
      },
      "source": [
        "# Install libraries\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorflow-model-optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZckCvXhKdR7"
      },
      "source": [
        "# Imports\n",
        "import tempfile\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# TensorFlow and Keras Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# We need to import the optimisation library for TensorFlow & Keras\n",
        "import tensorflow_model_optimization as tfmot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsG4zilFX7Aq"
      },
      "source": [
        "# Helper Function: Evaluate a TensorFlow Lite model on the test dataset (we will need this later)\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_images):\n",
        "    \n",
        "    # Status prints (uncomment if you need)\n",
        "    # if i % 1000 == 0:\n",
        "      # print(f'Evaluated on {n=i} results so far.')\n",
        "    \n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "\n",
        "  # Status prints (uncomment if you need)\n",
        "  # print('\\n')\n",
        "  \n",
        "  # Compare prediction results with ground truth labels to calculate accuracy\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  \n",
        "  \n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSL2t8-SW4Sj"
      },
      "source": [
        "## MNIST\n",
        "We will used the MNIST example to understand the quantisation pipeline in TensorFlow & Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2upPRmNqK64I"
      },
      "source": [
        "We will implement a small neural network (**without quantisation**) and train it on the MNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxGTg64EKtC6"
      },
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "\n",
        "# Train the digit classification model\n",
        "# We have to compile the model first (TensorFlow & Keras typical procedure)\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "# Let's print the model summary (the structure of our model)\n",
        "model.summary()\n",
        "\n",
        "# Fit the model to data\n",
        "model.fit(train_images, train_labels, epochs=1, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS_WwGNKPhvl"
      },
      "source": [
        "Let's now build the same small neural network, **but now with quantisation**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0fq0EC_PAAx"
      },
      "source": [
        "# We create an object that will allow us to apply quantisation to our previous model\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# TensorFlow & Keras now \"understands\" that we have a model with quantisation\n",
        "quant_model = quantize_model(model)\n",
        "\n",
        "# We must recompile this model (TensorFlow & Keras typical procedure)\n",
        "quant_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "# Let's print the model summary (the structure of our model)\n",
        "# Please notice that all layers are now prefixed by \"quant\"\n",
        "quant_model.summary()\n",
        "\n",
        "# Fit the quantised model to data\n",
        "quant_model.fit(train_images, train_labels, epochs=1, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXs6AhWDRnmM"
      },
      "source": [
        "Let's compare the accuracy of both models and check if there is any difference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krk3qkaESAwg"
      },
      "source": [
        "# Evaluate the accuracy of the model without quantisation\n",
        "_, baseline_model_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "# Evaluate the accuracy of the model with quantisation\n",
        "_, quant_model_accuracy = quant_model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "# Print these values in the Terminal\n",
        "print('Model with no quantisation Test Accuracy:', baseline_model_accuracy)\n",
        "print('Model with quantisation Test Accuracy:', quant_model_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7nc8e6sY1f8"
      },
      "source": [
        "Let's go a little bit further and create a quantised TensorFlow Lite model with a slightly *aggressive* quantisation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kqarceHZn0L"
      },
      "source": [
        "# We need to create an object that allows us to convert the quantised model from TensorFlow & Keras\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(quant_model)\n",
        "\n",
        "# We define a \"default\" optimisation settings \n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Let's now convert our model from TensorFlow & Keras to TensorFlow Lite\n",
        "quantized_tflite_model = converter.convert()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4iLPp-7aW9P"
      },
      "source": [
        "Let's check if accuracy persists for the sake of sanity:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsLWQbquaPsj"
      },
      "source": [
        "# We define an interpreter (it is an object that enables us to perform inference)\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Let's use our helper function (remember it?) to evaluate accuracy on this object\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "# Print accuracy values in the Terminal\n",
        "print('Quantised TensorFlow Lite model Test Accuracy:', test_accuracy)\n",
        "print('Quantised TensorFlow model Test Accuracy:', quant_model_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3On_SGNUce8d"
      },
      "source": [
        "Let's create a TensorFlow Lite and see if this model is indeed smaller than the model with no quantisation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpwReUZvcf2R"
      },
      "source": [
        "# Create float TensorFlow Lite model (from the model with no quantisation)\n",
        "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "float_tflite_model = float_converter.convert()\n",
        "\n",
        "\n",
        "# Measure sizes of models\n",
        "# Create temporary files with the contents of both the float and quantised models\n",
        "# Float File\n",
        "_, float_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "# Quantised File\n",
        "_, quant_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "\n",
        "# Write these files to disk\n",
        "# Float File\n",
        "with open(float_file, 'wb') as f:\n",
        "  f.write(float_tflite_model)\n",
        "\n",
        "# Quantised File\n",
        "with open(quant_file, 'wb') as f:\n",
        "  f.write(quantized_tflite_model)\n",
        "\n",
        "\n",
        "# Print the sizes of these models in the Terminal\n",
        "print(\"Float model in MB:\", os.path.getsize(float_file) / float(2**20))\n",
        "print(\"Quantised model in MB:\", os.path.getsize(quant_file) / float(2**20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NMkbtrjdlch"
      },
      "source": [
        "## Fashion-MNIST\n",
        "Let's see if we can apply our knowledge in a much more practical example, using Fashion-MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8l38V9WeZCn"
      },
      "source": [
        "We will implement a small neural network (**without quantisation**) and train it on the MNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6LLHCHVeZCv"
      },
      "source": [
        "# Load Fashion-MNIST dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "\n",
        "# Train the digit classification model\n",
        "# We have to compile the model first (TensorFlow & Keras typical procedure)\n",
        "# Your code here\n",
        "\n",
        "# Let's print the model summary (the structure of our model)\n",
        "# Your code here\n",
        "\n",
        "# Fit the model to data\n",
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIooYDROeZCx"
      },
      "source": [
        "Let's now build the same small neural network, **but now with quantisation**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b31IPvVueZCx"
      },
      "source": [
        "# We create an object that will allow us to apply quantisation to our previous model\n",
        "quantize_model = # Your code here\n",
        "\n",
        "# TensorFlow & Keras now \"understands\" that we have a model with quantisation\n",
        "quant_model = # Your code here\n",
        "\n",
        "# We must recompile this model (TensorFlow & Keras typical procedure)\n",
        "# Your code here\n",
        "\n",
        "# Let's print the model summary (the structure of our model)\n",
        "# Please notice that all layers are now prefixed by \"quant\"\n",
        "# Your code here\n",
        "\n",
        "# Fit the quantised model to data\n",
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tq8Ssm3eZCy"
      },
      "source": [
        "Let's compare the accuracy of both models and check if there is any difference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XixEwtejeZCy"
      },
      "source": [
        "# Evaluate the accuracy of the model without quantisation\n",
        "_, baseline_model_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "# Evaluate the accuracy of the model with quantisation\n",
        "_, quant_model_accuracy = quant_model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "# Print these values in the Terminal\n",
        "print('Model with no quantisation Test Accuracy:', baseline_model_accuracy)\n",
        "print('Model with quantisation Test Accuracy:', quant_model_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULZHNvcYeZCz"
      },
      "source": [
        "Let's go a little bit further and create a quantised TensorFlow Lite model with a slightly *aggressive* quantisation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySy2TrDTeZCz"
      },
      "source": [
        "# We need to create an object that allows us to convert the quantised model from TensorFlow & Keras\n",
        "converter = # Your code here\n",
        "\n",
        "# We define a \"default\" optimisation settings \n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Let's now convert our model from TensorFlow & Keras to TensorFlow Lite\n",
        "quantized_tflite_model = # Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRvhHonJeZCz"
      },
      "source": [
        "Let's check if accuracy persists for the sake of sanity:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teSdWu_yeZC0"
      },
      "source": [
        "# We define an interpreter (it is an object that enables us to perform inference)\n",
        "interpreter = # Your code here\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Let's use our helper function (remember it?) to evaluate accuracy on this object\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "# Print accuracy values in the Terminal\n",
        "print('Quantised TensorFlow Lite model Test Accuracy:', test_accuracy)\n",
        "print('Quantised TensorFlow model Test Accuracy:', quant_model_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_GusKxPeZC1"
      },
      "source": [
        "Let's create a TensorFlow Lite and see if this model is indeed smaller than the model with no quantisation (results should be similar to MNIST example since we did not change the architecture of the model):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dRhdB8IeZC1"
      },
      "source": [
        "# Create float TensorFlow Lite model (from the model with no quantisation)\n",
        "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "float_tflite_model = float_converter.convert()\n",
        "\n",
        "\n",
        "# Measure sizes of models\n",
        "# Create temporary files with the contents of both the float and quantised models\n",
        "# Float File\n",
        "_, float_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "# Quantised File\n",
        "_, quant_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "\n",
        "# Write these files to disk\n",
        "# Float File\n",
        "with open(float_file, 'wb') as f:\n",
        "  f.write(float_tflite_model)\n",
        "\n",
        "# Quantised File\n",
        "with open(quant_file, 'wb') as f:\n",
        "  f.write(quantized_tflite_model)\n",
        "\n",
        "\n",
        "# Print the sizes of these models in the Terminal\n",
        "print(\"Float model in MB:\", os.path.getsize(float_file) / float(2**20))\n",
        "print(\"Quantised model in MB:\", os.path.getsize(quant_file) / float(2**20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMzinhQyBZaO"
      },
      "source": [
        "# Model Pruning in TensorFlow & Keras\n",
        "\n",
        "Theory:\n",
        "https://www.tensorflow.org/model_optimization/guide/pruning?hl=en\n",
        "\n",
        "Code and Exercises adapted from:\n",
        "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras?hl=en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_v1WrVakDKK"
      },
      "source": [
        "## Setup\n",
        "Let's start with the setup of our development environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LanSxnmiBauR"
      },
      "source": [
        "# Install libraries\n",
        "!pip install -q tensorflow-model-optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5riLeU8kflj"
      },
      "source": [
        "# Imports\n",
        "import tempfile\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# TensorFlow & Keras Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# TensorFlow & Keras Optimisation Libraries\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCeElPH0lBaq"
      },
      "source": [
        "# Helper Funtion: Compress the models via gzip and measure the zipped size\n",
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsWUEHS1lKEJ"
      },
      "source": [
        "# Helper Function: Evaluate the TensorFlow Lite model on the test dataset\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on ever y image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_images):\n",
        "    # if i % 1000 == 0:\n",
        "      # print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    \n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with the model's input data format\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  # print('\\n')\n",
        "  \n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  \n",
        "  \n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJUpZqg2mIjx"
      },
      "source": [
        "## MNIST\n",
        "We will used the MNIST example to understand the pruning pipeline in TensorFlow & Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmgVab2fmmwf"
      },
      "source": [
        "We start by training a model without pruning, on the MNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct_GRFFXmgPv"
      },
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "\n",
        "# Define the model architecture.\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "\n",
        "# Train the digit classification model\n",
        "# We have to compile the model first\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "# Let's print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Let's fit the model to our data\n",
        "model.fit(train_images, train_labels, epochs=5, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m0OSjYunR9l"
      },
      "source": [
        "# Let's check the model accuracy\n",
        "_, baseline_model_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "# Print the value in the Terminal\n",
        "print('Baseline Model Test Accuracy:', baseline_model_accuracy)\n",
        "\n",
        "\n",
        "# Save the model for later usage\n",
        "_, keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
        "print('Saved baseline model to:', keras_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-0N9zyEot1s"
      },
      "source": [
        "Let's apply a **pruning strategy** and retrain the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDeC777Boptg"
      },
      "source": [
        "# Create an object that enables us to apply pruning\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "\n",
        "# Compute end step to finish pruning after 5 epochs\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "validation_split = 0.1\n",
        "\n",
        "# Number of training images\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Define model for pruning\n",
        "# In this example, you start the model with 50% sparsity (50% zeros in weights) and end with 80% sparsity\n",
        "# We define the prunning parameters with a dictionary object\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                               final_sparsity=0.80,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "\n",
        "\n",
        "# Apply pruning to our baseline model\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "\n",
        "# As you already know, we have to recompile the model again\n",
        "model_for_pruning.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "# Let's print the summary of this model\n",
        "model_for_pruning.summary()\n",
        "\n",
        "\n",
        "# Callbacks are TensorFlow & Keras functions that we want to call during the training process\n",
        "# We create a logdir for one of our callbacks\n",
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "# tfmot.sparsity.keras.UpdatePruningStep is required during training\n",
        "# tfmot.sparsity.keras.PruningSummaries provides logs for tracking progress and debugging.\n",
        "callbacks = [tfmot.sparsity.keras.UpdatePruningStep(), tfmot.sparsity.keras.PruningSummaries(log_dir=logdir)]\n",
        "\n",
        "# Fit this model to our data\n",
        "model_for_pruning.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMqnvW4Brrc4"
      },
      "source": [
        "# Let's evaluate the accuracy of this model\n",
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "\n",
        "# Print the accuracy values on the Terminal\n",
        "print('Baseline Model Test Accuracy:', baseline_model_accuracy) \n",
        "print('Pruned Model Test Accuracy:', model_for_pruning_accuracy)\n",
        "\n",
        "\n",
        "# Let's check the evolution of the sparsity of our models\n",
        "%tensorboard --logdir={logdir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cDl2Ml7s_av"
      },
      "source": [
        "Let's use TensorFlow Lite to reduce the size of our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbSlBwm9tlev"
      },
      "source": [
        "# According to TensorFlow Lite documentation\n",
        "# Both tfmot.sparsity.keras.strip_pruning and applying a standard compression algorithm (e.g. via gzip) are necessary to see the compression benefits of pruning\n",
        "# strip_pruning is necessary since it removes every tf.Variable that pruning only needs during training, which would otherwise add to model size during inference\n",
        "# Applying a standard compression algorithm is necessary since the serialized weight matrices are the same size as they were before pruning \n",
        "# However, pruning makes most of the weights zeros, which is added redundancy that algorithms can utilize to further compress the model\n",
        "\n",
        "\n",
        "# Create an object to export the model\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "\n",
        "# Create a temporary file to save the contents of this model\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "\n",
        "# Save this model into disk\n",
        "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwac9lJXuPgm"
      },
      "source": [
        "# Similarly to the previous tutorial on quantisation, we have to create TensorFlow Lite objects to convert our model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "pruned_tflite_model = converter.convert()\n",
        "\n",
        "\n",
        "# Create a temporary file to save the contents of this model\n",
        "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "\n",
        "# Save this model into disk\n",
        "with open(pruned_tflite_file, 'wb') as f:\n",
        "  f.write(pruned_tflite_model)\n",
        "\n",
        "print('Saved pruned TFLite model to:', pruned_tflite_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAQW2A6Ruu1Y"
      },
      "source": [
        "# Let's use one of our helper functions to evaluate the size of our models\n",
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivz2HRby7tuU"
      },
      "source": [
        "## CIFAR-10\n",
        "Let's now replicate this strategy using a RGB image dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpL5-AdD8E6C"
      },
      "source": [
        "# Load CIFAR-10 dataset\n",
        "cifar10 = tf.keras.datasets.cifar10\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = # Your code here\n",
        "test_images = # Your code here\n",
        "\n",
        "\n",
        "# Define the model architecture.\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(32, 32, 3)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "\n",
        "# Train the digit classification model\n",
        "# We have to compile the model first\n",
        "# Your code here\n",
        "\n",
        "# Let's print the model summary\n",
        "# Your code here\n",
        "\n",
        "# Let's fit the model to our data\n",
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7-ZOE-h8E6K"
      },
      "source": [
        "# Let's check the model accuracy\n",
        "_, baseline_model_accuracy = # Your code here\n",
        "\n",
        "# Print the value in the Terminal\n",
        "print('Baseline Model Test Accuracy:', baseline_model_accuracy)\n",
        "\n",
        "\n",
        "# Save the model for later usage\n",
        "# Create a temporary file to save this model contents\n",
        "_, keras_file = tempfile.mkstemp('.h5')\n",
        "\n",
        "# Save model\n",
        "# Your code here (hint: use the function tf.keras.models.save_model) \n",
        "\n",
        "print('Saved baseline model to:', keras_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awm3_E2k8E6K"
      },
      "source": [
        "Let's apply a **pruning strategy** and retrain the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqctSQQ08E6L"
      },
      "source": [
        "# Create an object that enables us to apply pruning\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "\n",
        "# Compute end step to finish pruning after 5 epochs\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "validation_split = 0.1\n",
        "\n",
        "# Number of training images\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Define model for pruning\n",
        "# In this example, you start the model with 50% sparsity (50% zeros in weights) and end with 80% sparsity\n",
        "# We define the prunning parameters with a dictionary object\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                               final_sparsity=0.80,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "\n",
        "\n",
        "# Apply pruning to our baseline model\n",
        "model_for_pruning = # Your code here (hint: use the prune_low_magnitude function)\n",
        "\n",
        "\n",
        "# As you already know, we have to recompile the model again\n",
        "model_for_pruning.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "# Let's print the summary of this model\n",
        "model_for_pruning.summary()\n",
        "\n",
        "\n",
        "# Callbacks are TensorFlow & Keras functions that we want to call during the training process\n",
        "# We create a logdir for one of our callbacks\n",
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "# tfmot.sparsity.keras.UpdatePruningStep is required during training\n",
        "# tfmot.sparsity.keras.PruningSummaries provides logs for tracking progress and debugging.\n",
        "callbacks = # Your code here (check the previous example)\n",
        "\n",
        "# Fit this model to our data\n",
        "# Your code here (check the previous example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcnE5kOx8E6L"
      },
      "source": [
        "# Let's evaluate the accuracy of this model\n",
        "_, model_for_pruning_accuracy = # Your code here (check the previous example)\n",
        "\n",
        "\n",
        "# Print the accuracy values on the Terminal\n",
        "print('Baseline Model Test Accuracy:', baseline_model_accuracy) \n",
        "print('Pruned Model Test Accuracy:', model_for_pruning_accuracy)\n",
        "\n",
        "\n",
        "# Let's check the evolution of the sparsity of our models\n",
        "%tensorboard --logdir={logdir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k92pBHBf8E6M"
      },
      "source": [
        "Let's use TensorFlow Lite to reduce the size of our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXWsQ0CW8E6M"
      },
      "source": [
        "# According to TensorFlow Lite documentation\n",
        "# Both tfmot.sparsity.keras.strip_pruning and applying a standard compression algorithm (e.g. via gzip) are necessary to see the compression benefits of pruning\n",
        "# strip_pruning is necessary since it removes every tf.Variable that pruning only needs during training, which would otherwise add to model size during inference\n",
        "# Applying a standard compression algorithm is necessary since the serialized weight matrices are the same size as they were before pruning \n",
        "# However, pruning makes most of the weights zeros, which is added redundancy that algorithms can utilize to further compress the model\n",
        "\n",
        "\n",
        "# Create an object to export the model\n",
        "model_for_export = # Your code here (hint: use the tfmot.sparsity.keras.strip_pruning function)\n",
        "\n",
        "# Create a temporary file to save the contents of this model\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "\n",
        "# Save this model into disk\n",
        "# Your code here (hint: use the tf.keras.models.save_model function)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvJ8ZI7C8E6M"
      },
      "source": [
        "# Similarly to the previous tutorial on quantisation, we have to create TensorFlow Lite objects to convert our model\n",
        "converter = # Your code here (hint: use the tf.lite.TFLiteConverter.from_keras_model function)\n",
        "pruned_tflite_model = converter.convert()\n",
        "\n",
        "\n",
        "# Create a temporary file to save the contents of this model\n",
        "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "\n",
        "# Save this model into disk\n",
        "with open(pruned_tflite_file, 'wb') as f:\n",
        "  f.write(pruned_tflite_model)\n",
        "\n",
        "print('Saved pruned TFLite model to:', pruned_tflite_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN98DxSE8E6N"
      },
      "source": [
        "# Let's use one of our helper functions to evaluate the size of our models\n",
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySZIb1DUBbx7"
      },
      "source": [
        "# What if we want to jointly use pruning and quantisation?\n",
        "Theory: https://www.tensorflow.org/model_optimization/guide/pruning?hl=en\n",
        "\n",
        "Code and Exercises adapted from: https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras?hl=en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr2PNkkMCSbR"
      },
      "source": [
        "## Setup\n",
        "Let's start with the setup of our development environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk-ZPgc4CSbY"
      },
      "source": [
        "# Install libraries\n",
        "!pip install -q tensorflow-model-optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BUqoTg0CSbZ"
      },
      "source": [
        "# Imports\n",
        "import tempfile\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# TensorFlow & Keras Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# TensorFlow & Keras Optimisation Libraries\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QheUuG3CSbZ"
      },
      "source": [
        "# Helper Funtion: Compress the models via gzip and measure the zipped size\n",
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f93gyD_oCSba"
      },
      "source": [
        "# Helper Function: Evaluate the TensorFlow Lite model on the test dataset\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on ever y image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_images):\n",
        "    # if i % 1000 == 0:\n",
        "      # print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    \n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with the model's input data format\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  # print('\\n')\n",
        "  \n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  \n",
        "  \n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pdDV9UNCSba"
      },
      "source": [
        "## MNIST\n",
        "We will used the MNIST example to understand the pruning pipeline in TensorFlow & Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXayKmjlCSbb"
      },
      "source": [
        "We start by training a model without pruning, on the MNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7_DAwMXCSbb"
      },
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "\n",
        "# Define the model architecture.\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "\n",
        "# Train the digit classification model\n",
        "# We have to compile the model first\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "# Let's print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Let's fit the model to our data\n",
        "model.fit(train_images, train_labels, epochs=5, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbCf4GxwCSbc"
      },
      "source": [
        "# Let's check the model accuracy\n",
        "_, baseline_model_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "# Print the value in the Terminal\n",
        "print('Baseline Model Test Accuracy:', baseline_model_accuracy)\n",
        "\n",
        "\n",
        "# Save the model for later usage\n",
        "_, keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
        "print('Saved baseline model to:', keras_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAb5mxRaCSbc"
      },
      "source": [
        "Let's apply a **pruning strategy** and retrain the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF0uHoEPCSbd"
      },
      "source": [
        "# Create an object that enables us to apply pruning\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "\n",
        "# Compute end step to finish pruning after 5 epochs\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "validation_split = 0.1\n",
        "\n",
        "# Number of training images\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Define model for pruning\n",
        "# In this example, you start the model with 50% sparsity (50% zeros in weights) and end with 80% sparsity\n",
        "# We define the prunning parameters with a dictionary object\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                               final_sparsity=0.80,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "\n",
        "\n",
        "# Apply pruning to our baseline model\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "\n",
        "# As you already know, we have to recompile the model again\n",
        "model_for_pruning.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "# Let's print the summary of this model\n",
        "model_for_pruning.summary()\n",
        "\n",
        "\n",
        "# Callbacks are TensorFlow & Keras functions that we want to call during the training process\n",
        "# We create a logdir for one of our callbacks\n",
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "# tfmot.sparsity.keras.UpdatePruningStep is required during training\n",
        "# tfmot.sparsity.keras.PruningSummaries provides logs for tracking progress and debugging.\n",
        "callbacks = [tfmot.sparsity.keras.UpdatePruningStep(), tfmot.sparsity.keras.PruningSummaries(log_dir=logdir)]\n",
        "\n",
        "# Fit this model to our data\n",
        "model_for_pruning.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUeyJCEeCSbe"
      },
      "source": [
        "# Let's evaluate the accuracy of this model\n",
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "\n",
        "# Print the accuracy values on the Terminal\n",
        "print('Baseline Model Test Accuracy:', baseline_model_accuracy) \n",
        "print('Pruned Model Test Accuracy:', model_for_pruning_accuracy)\n",
        "\n",
        "\n",
        "# Let's check the evolution of the sparsity of our models\n",
        "%tensorboard --logdir={logdir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnIcarGaCSbe"
      },
      "source": [
        "Let's use TensorFlow Lite to reduce the size of our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzqNjyGUCSbf"
      },
      "source": [
        "# According to TensorFlow Lite documentation\n",
        "# Both tfmot.sparsity.keras.strip_pruning and applying a standard compression algorithm (e.g. via gzip) are necessary to see the compression benefits of pruning\n",
        "# strip_pruning is necessary since it removes every tf.Variable that pruning only needs during training, which would otherwise add to model size during inference\n",
        "# Applying a standard compression algorithm is necessary since the serialized weight matrices are the same size as they were before pruning \n",
        "# However, pruning makes most of the weights zeros, which is added redundancy that algorithms can utilize to further compress the model\n",
        "\n",
        "\n",
        "# Create an object to export the model\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "\n",
        "# Create a temporary file to save the contents of this model\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "\n",
        "# Save this model into disk\n",
        "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNixVFptCSbf"
      },
      "source": [
        "# Similarly to the previous tutorial on quantisation, we have to create TensorFlow Lite objects to convert our model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "pruned_tflite_model = converter.convert()\n",
        "\n",
        "\n",
        "# Create a temporary file to save the contents of this model\n",
        "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "\n",
        "# Save this model into disk\n",
        "with open(pruned_tflite_file, 'wb') as f:\n",
        "  f.write(pruned_tflite_model)\n",
        "\n",
        "print('Saved pruned TFLite model to:', pruned_tflite_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjhkoLoxCSbf"
      },
      "source": [
        "# Let's use one of our helper functions to evaluate the size of our models\n",
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLUrE415CpEH"
      },
      "source": [
        "Let's now observe if we can use quantisation after pruning to reduce the size of our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfiJQgiDCjeD"
      },
      "source": [
        "# Let's, once again, create a TensorFlow Lite object to convert our models\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_and_pruned_tflite_model = converter.convert()\n",
        "\n",
        "\n",
        "# We create a temporary file to save the contents of this model\n",
        "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "\n",
        "# Save the model into disk\n",
        "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
        "  f.write(quantized_and_pruned_tflite_model)\n",
        "\n",
        "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
        "\n",
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkVK1p_rFRxl"
      },
      "source": [
        "Let's check if the accuracy of this compressed model is similar to the baseline model, for the sake of sanity:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFgVRrxwE0CE"
      },
      "source": [
        "# We create an object to perform inference (remember the Quantisation tutorial?)\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# We use the helper function to evaluate this model\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
        "print('Pruned TF test accuracy:', model_for_pruning_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YzVPnq3HPmH"
      },
      "source": [
        "## Challenge: can you pick up one (or more) dataset(s) and build the entire pipeline yourself?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bSg2y9IHrFq"
      },
      "source": [
        "# Start your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q460FZ3kHuQz"
      },
      "source": [
        "# More tutorials, exercises and readings:\n",
        "\n",
        "\n",
        "1.   [Model Quantization Methods In TensorFlow Lite By Bhavika Kanani](https://studymachinelearning.com/model-quantization-methods-in-tensorflow-lite/)\n",
        "\n",
        "2.   [TensorFlow model optimization guide](https://www.tensorflow.org/model_optimization/guide?hl=en)\n",
        "\n",
        "3.   [Speeding Up Deep Learning Inference Using TensorFlow, ONNX, and NVIDIA TensorRT](https://developer.nvidia.com/blog/speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt/)\n",
        "\n",
        "4.   [PyTorch: Pruning Tutorial by Michela Paganini](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#extending-torch-nn-utils-prune-with-custom-pruning-functions)\n",
        "\n",
        "5.   [PyTorch: Dynamic Quantization on an LSTM Word Language Model by James Reed](https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html)\n",
        "\n",
        "6.   [PyTorch: Dynamic Quantization on BERT by Jianyu Huang](https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html)\n",
        "\n",
        "7.   [PyTorch: Quantized Transfer Learning for Computer Vision Tutorial by Zafar Takhirov](https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html)\n",
        "\n",
        "8.   [PyTorch: Static Quantization with Eager Mode in PyTorch](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)\n",
        "\n",
        "9.   [PyTorch: Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)\n",
        "\n",
        "10.  [Brevitas: a PyTorch research library for quantization-aware training (QAT)](https://github.com/Xilinx/brevitas)"
      ]
    }
  ]
}